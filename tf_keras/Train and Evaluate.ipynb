{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate with Keras\n",
    "\n",
    "This guide covers training, evaluation, and prediction (inference) models in Tensorflow 2.0 in two board situations:\n",
    "- When using built-in APIs for training & validation (such as `model.fit(), model.evaluate(), model.predict()`). This is covered in the section **\"Using built-in training and evaluation loops\"**.\n",
    "- When writing custom loops from scratch using eager execution and the `GradientTape` object. This is covered in the section **\"Writing your own training & evaluation loops from scratch\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Using built-in training & evaluation loops\n",
    "\n",
    "When passing data to the built-in training loops of a model, you should either use **Numpy arrays** or **tf.data Dataset** objects.\n",
    "\n",
    "### API overview: a first end-to-end example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inputs and outputs\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to test this example\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data (there are Numpy arrarys\n",
    "x_train = x_train.reshape(x_train.shape[0],784).astype('float32')/255\n",
    "x_test = x_test.reshape(x_test.shape[0],784).astype('float32')/255\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse 10000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (50000, 784))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape, x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific the training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(), # optimizer\n",
    "             # loss function to minimize\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              # list of metrics to monitor\n",
    "              metrics=['sparse_categorical_accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by slicing the data into \"batches\" of size \"batch_size\", and repeatedly interating over the entire dataset for a given number of \"epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit the model on training data\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 3s 53us/sample - loss: 0.0670 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.0997 - val_sparse_categorical_accuracy: 0.9716\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 3s 52us/sample - loss: 0.0568 - sparse_categorical_accuracy: 0.9835 - val_loss: 0.1183 - val_sparse_categorical_accuracy: 0.9671\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 3s 52us/sample - loss: 0.0505 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.1216 - val_sparse_categorical_accuracy: 0.9687\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 2s 49us/sample - loss: 0.0435 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.0999 - val_sparse_categorical_accuracy: 0.9738\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 2s 50us/sample - loss: 0.0376 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.1074 - val_sparse_categorical_accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "print(\"# Fit the model on training data\")\n",
    "history = model.fit(x_train, y_train,\n",
    "                   batch_size=64,\n",
    "                   epochs=5,\n",
    "                   # we pass some validation for monitoring validation loss and metrics at the end of each epoch\n",
    "                   validation_data=(x_val, y_val)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History dict:\n",
      " {'loss': [0.06696626014888286, 0.05678928085118532, 0.05046863056123257, 0.04350630847066641, 0.037609205454066395], 'sparse_categorical_accuracy': [0.97956, 0.9835, 0.98472, 0.98692, 0.98864], 'val_loss': [0.09973060534149408, 0.11829584560794756, 0.12163342349436135, 0.09987891764855013, 0.10743547966787592], 'val_sparse_categorical_accuracy': [0.9716, 0.9671, 0.9687, 0.9738, 0.9735]}\n"
     ]
    }
   ],
   "source": [
    "print(\"History dict:\\n\", history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned history object holds a record of the loss values and metric values during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluate on test data\n",
      "10000/10000 [==============================] - 0s 11us/sample - loss: 0.0942 - sparse_categorical_accuracy: 0.9738\n",
      "Test loss, test acc:  [0.09424186117689241, 0.9738]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test data using `evaluate`\n",
    "print('# Evaluate on test data')\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('Test loss, test acc: ', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions for 3 samples\n",
      "predictions shape:  (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# generate predictions (probabilities -- the output of the last layer)\n",
    "# on the new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape: \", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-17.87567  , -13.672206 ,  -7.631255 ,  -3.1017962, -31.413145 ,\n",
       "        -12.665701 , -30.023172 ,   7.1734614, -10.685866 ,  -8.515322 ],\n",
       "       [-30.172102 ,  -8.60126  ,   8.508872 , -14.9676695, -39.718273 ,\n",
       "        -15.821878 , -18.493551 , -24.362415 , -11.735703 , -47.791016 ],\n",
       "       [-13.620418 ,   3.658851 ,  -6.3016615,  -8.529528 , -10.229552 ,\n",
       "        -11.651248 ,  -8.606226 ,  -6.8406477,  -4.7101274, -14.5578   ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying a loss, metrics, and an optimizer\n",
    "To train a model with `fit`, you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.\n",
    "\n",
    "You pass these to the model as arguments to the `compile()` method:\n",
    "\n",
    "`\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.sparse_categorical_accuracy])\n",
    "`\n",
    "\n",
    "Note that if you're satisfied with the default settings, in many cases the optimizer, loss, and metrics can be specified via string identifiers as a shortcut:\n",
    "\n",
    "`\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['sparse_categorical_accuracy'])`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later reuse, let's put our model definition and compile step in functions; we will call them several times across different examples in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name='digits')\n",
    "    x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = layers.Dense(10, name='predictions')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many built-in optimizers, losses, and metrics ae available\n",
    "\n",
    "In general, you won't have to create from scratch your own losses, metrics, or optimizers, because what you need is likely already part of the Keras API:\n",
    "\n",
    "Optimizers:\n",
    "\n",
    "`SGD() (with or without momentum)\n",
    "RMSprop()\n",
    "Adam()\n",
    "etc.`\n",
    "\n",
    "Losses:\n",
    "\n",
    "`MeanSquaredError()\n",
    "KLDivergence()\n",
    "CosineSimilarity()\n",
    "etc.`\n",
    "\n",
    "Metrics:\n",
    "\n",
    "`AUC()\n",
    "Precision()\n",
    "Recall()\n",
    "etc.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
